{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genism_models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJMpggsis9EvUu2W2x541c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushyag1/NLPClass/blob/master/genism_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohFbJMDhjdZi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6fyKyvwjgrJ"
      },
      "source": [
        "## Use the ‘abcnews-date-text.csv’ file.\n",
        "• Load the data\n",
        "\n",
        "• Use data_text = data[[‘headline_text’]], ‘data_text[‘index’]=data_text.index’,\n",
        "‘documents=data_text’.\n",
        "\n",
        "• Import the following libraries:\n",
        "o import gensim\n",
        "o from gensim.utils import simple_preprocess\n",
        "o from gensim.parsing.preprocessing import STOPWORDS\n",
        "o from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "o from nltk.stem.porter import *\n",
        "o import numpy as np\n",
        "\n",
        "• Load ‘wordnet’\n",
        "\n",
        "• Tokenize and lemmatize the text\n",
        "\n",
        "• Create a dictionary as ‘dictionary=genism.corpora.Dictionary(processed_docs)’\n",
        "\n",
        "• Filter extremes from dictionary\n",
        "\n",
        "• Create a ‘bow_corpus’ using ‘doc2bow(doc)’ for doc in processed document\n",
        "\n",
        "• From genism import corpora, models and use Tfidf for the ‘bow_corpus’\n",
        "\n",
        "• Create a lda_model as ‘gensim.models.LdaMulticore(bow_corpus,\n",
        "num_topics=10, id2word=dictionary, passes=2, workers=2)’\n",
        "\n",
        "• Print the topics using BOW\n",
        "\n",
        "• Run LDA using lda_model_tfidf as ‘gensim.models.LdaMulticore(corpus_tfidf,\n",
        "num_topics=10, id2word=dictionary, passes=2, workers=4)’\n",
        "\n",
        "• Print the topics using Tfidf\n",
        "\n",
        "• Evaluate the performance by classifying sample document using LDA BOW\n",
        "\n",
        "• Evaluate the performance by classifying sample document using LDA Tfidf\n",
        "\n",
        "• Test the model on unseen document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmBEooFmjpPP"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
        "data_text = data[['headline_text']]\n",
        "data_text['index'] = data_text.index\n",
        "documents = data_text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bQeRqoVkkuc",
        "outputId": "18bb4a46-d3aa-434a-fb8b-b8d5c157759b"
      },
      "source": [
        "print(len(documents))\n",
        "print(documents[:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1186018\n",
            "                                       headline_text  index\n",
            "0  aba decides against community broadcasting lic...      0\n",
            "1     act fire witnesses must be aware of defamation      1\n",
            "2     a g calls for infrastructure protection summit      2\n",
            "3           air nz staff in aust strike for pay rise      3\n",
            "4      air nz strike to affect australian travellers      4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhaHInTTkkzM",
        "outputId": "25ba8cb4-8e4b-4e7d-e352-5d437309ca45"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yOJ-hOekk1R"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVeD17gEkk4o",
        "outputId": "92ca788a-72bc-4e19-eb47-4d08470caf8c"
      },
      "source": [
        "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDjIDBakk2n5",
        "outputId": "acbb0bd0-9b79-4e3a-f808-0ed379979729"
      },
      "source": [
        "processed_docs = documents['headline_text'].map(preprocess)\n",
        "processed_docs[:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            [decid, communiti, broadcast, licenc]\n",
              "1                               [wit, awar, defam]\n",
              "2           [call, infrastructur, protect, summit]\n",
              "3                      [staff, aust, strike, rise]\n",
              "4             [strike, affect, australian, travel]\n",
              "5               [ambiti, olsson, win, tripl, jump]\n",
              "6           [antic, delight, record, break, barca]\n",
              "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
              "8            [aust, address, secur, council, iraq]\n",
              "9                         [australia, lock, timet]\n",
              "Name: headline_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUzVkId3lAfc",
        "outputId": "06dd8778-fac9-4b32-ab22-7d228bb05654"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 broadcast\n",
            "1 communiti\n",
            "2 decid\n",
            "3 licenc\n",
            "4 awar\n",
            "5 defam\n",
            "6 wit\n",
            "7 call\n",
            "8 infrastructur\n",
            "9 protect\n",
            "10 summit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGxmsrsRlORU"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyfaEKE3l1ja",
        "outputId": "411d2bf2-4a77-42df-a9c1-38d5919ceb29"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[4310]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(162, 1), (240, 1), (292, 1), (589, 1), (838, 1), (3567, 1), (3568, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ylmycnlU8c",
        "outputId": "d96163d2-598a-4468-a448-6777bb048b21"
      },
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                               dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 162 (\"govt\") appears 1 time.\n",
            "Word 240 (\"group\") appears 1 time.\n",
            "Word 292 (\"vote\") appears 1 time.\n",
            "Word 589 (\"local\") appears 1 time.\n",
            "Word 838 (\"want\") appears 1 time.\n",
            "Word 3567 (\"compulsori\") appears 1 time.\n",
            "Word 3568 (\"ratepay\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oowANpKomDda",
        "outputId": "63a91df1-b6f8-4dc5-9539-e1b43baf15d5"
      },
      "source": [
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "from pprint import pprint\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.5850076620505259),\n",
            " (1, 0.38947256567331934),\n",
            " (2, 0.4997099083387053),\n",
            " (3, 0.5063271308533074)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bm07LJslfLQ"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stqgsbkkn698",
        "outputId": "d0b35334-b79d-443c-a9a5-bed3f06e2b85"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.027*\"death\" + 0.019*\"canberra\" + 0.015*\"hospit\" + 0.010*\"water\" + 0.010*\"flood\" + 0.010*\"hobart\" + 0.010*\"reveal\" + 0.009*\"find\" + 0.009*\"take\" + 0.008*\"home\"\n",
            "Topic: 1 \n",
            "Words: 0.023*\"donald\" + 0.019*\"china\" + 0.016*\"island\" + 0.016*\"australian\" + 0.015*\"rise\" + 0.013*\"street\" + 0.013*\"fall\" + 0.012*\"show\" + 0.010*\"wall\" + 0.010*\"road\"\n",
            "Topic: 2 \n",
            "Words: 0.030*\"elect\" + 0.021*\"south\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"australia\" + 0.012*\"interview\" + 0.012*\"perth\" + 0.011*\"student\" + 0.011*\"school\" + 0.010*\"stori\"\n",
            "Topic: 3 \n",
            "Words: 0.027*\"market\" + 0.018*\"miss\" + 0.016*\"indigen\" + 0.015*\"victoria\" + 0.013*\"sydney\" + 0.012*\"beat\" + 0.012*\"citi\" + 0.012*\"search\" + 0.011*\"share\" + 0.011*\"price\"\n",
            "Topic: 4 \n",
            "Words: 0.024*\"charg\" + 0.020*\"murder\" + 0.019*\"australia\" + 0.018*\"melbourn\" + 0.016*\"world\" + 0.015*\"court\" + 0.014*\"face\" + 0.014*\"alleg\" + 0.013*\"test\" + 0.012*\"accus\"\n",
            "Topic: 5 \n",
            "Words: 0.022*\"chang\" + 0.017*\"jail\" + 0.014*\"rural\" + 0.013*\"busi\" + 0.013*\"govern\" + 0.012*\"break\" + 0.011*\"say\" + 0.011*\"drum\" + 0.010*\"climat\" + 0.010*\"concern\"\n",
            "Topic: 6 \n",
            "Words: 0.015*\"feder\" + 0.015*\"health\" + 0.014*\"bushfir\" + 0.014*\"farmer\" + 0.013*\"royal\" + 0.012*\"plan\" + 0.012*\"speak\" + 0.011*\"guilti\" + 0.010*\"commiss\" + 0.010*\"help\"\n",
            "Topic: 7 \n",
            "Words: 0.025*\"attack\" + 0.019*\"polic\" + 0.018*\"kill\" + 0.016*\"govern\" + 0.014*\"arrest\" + 0.014*\"tasmanian\" + 0.014*\"countri\" + 0.014*\"sentenc\" + 0.012*\"driver\" + 0.011*\"tell\"\n",
            "Topic: 8 \n",
            "Words: 0.039*\"trump\" + 0.025*\"queensland\" + 0.020*\"crash\" + 0.019*\"news\" + 0.017*\"die\" + 0.017*\"shoot\" + 0.016*\"coast\" + 0.015*\"dead\" + 0.013*\"time\" + 0.012*\"polic\"\n",
            "Topic: 9 \n",
            "Words: 0.020*\"warn\" + 0.020*\"nation\" + 0.016*\"peopl\" + 0.013*\"farm\" + 0.011*\"liber\" + 0.011*\"victorian\" + 0.011*\"leader\" + 0.010*\"parti\" + 0.010*\"state\" + 0.009*\"weather\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrav1EkEn95A"
      },
      "source": [
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSV5y9B5oAZB"
      },
      "source": [
        "processed_docs[4310]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97FP46rAowKW"
      },
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SpAZSMLo0Zc"
      },
      "source": [
        "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8eHKEgjo7Z9"
      },
      "source": [
        "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JyUb3Sco-xM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}