{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week11_HW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2wwaOg5RaWIenL0kOaZDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushyag1/NLPClass/blob/master/Movie_Review_MultinomialNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuvGm5BrhOg4"
      },
      "source": [
        "Use the movie_reviews.txt file.\n",
        "\n",
        "• Load the data\n",
        "\n",
        "• Change the txt into a csv file\n",
        "\n",
        "• Remove the ‘Unnamed’ column\n",
        "\n",
        "• Print the first 5 rows\n",
        "\n",
        "• Set X as movie_data[‘Summary’] and y as movie_data[‘Sentiment’]\n",
        "\n",
        "• Split the dataset into ‘docs_train’, ‘docs_test’, y_train, y_test. Test size is 20%\n",
        "\n",
        "• Initialize CountVectorizer:\n",
        "o movieVzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)\n",
        "Use all 25K words. Higher accuracy\n",
        "\n",
        "• Locate the word ‘screen’ and ‘Steven Seagal’ in the corpus\n",
        "\n",
        "• Determine the shape of ‘docs_train_counts\n",
        "\n",
        "• Convert raw frequency counts into Tfidf values\n",
        "\n",
        "• Using the fitted vectorizer and transformer, transform the test data\n",
        "\n",
        "• Use the Multinomial NB as a classifier\n",
        "\n",
        "• Predict the test set results and determine the accuracy\n",
        "\n",
        "• Provide the confusion matrix\n",
        "\n",
        "• Provide the classification report\n",
        "\n",
        "• Use GridSearchCV with a logistic regression to identify accuracy, the best\n",
        "estimator, score and parameter. Use ‘scoring=roc_auc’ and ‘cv=5’.\n",
        "\n",
        "• Provide the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXckQlevhZnq",
        "outputId": "538fa337-c1bc-4ee5-ec90-96179b11570c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szI_M2EPheNl"
      },
      "source": [
        "read_file = pd.read_csv (r'Movie_Reviews.txt')\n",
        "read_file.columns = ['Summary','Sentiment']\n",
        "read_file.to_csv (r'Movie_Reviews.csv', header=[\"Summary\", \"Sentiment\"])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy2xWCSEiCma",
        "outputId": "aedefd3b-32f2-4b0a-ec22-77a3847b563b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "movie_data = pd.read_csv('Movie_Reviews.csv')\n",
        "movie_data = movie_data.loc[:, ~movie_data.columns.str.contains('^Unnamed')]\n",
        "movie_data.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rock destined st century new conan going make ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gorgeously elaborate continuation lord ring tr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>effective tepid biopic</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sometimes like go movie fun wasabi good place ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>emerges something rare issue movie honest keen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Summary  Sentiment\n",
              "0  rock destined st century new conan going make ...          1\n",
              "1  gorgeously elaborate continuation lord ring tr...          1\n",
              "2                             effective tepid biopic          1\n",
              "3  sometimes like go movie fun wasabi good place ...          1\n",
              "4  emerges something rare issue movie honest keen...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h40h_2UmiJp4"
      },
      "source": [
        "X, y = movie_data['Summary'], movie_data['Sentiment']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JHDL64iMgD"
      },
      "source": [
        "# Split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "docs_train, docs_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size = 0.20, random_state = 12)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-vfitHdiOJR"
      },
      "source": [
        "# initialize CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000) # use top 3000 words only. 78.25% acc.\n",
        "# movieVzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize) # use all 25K words. Higher accuracy\n",
        "# fit and tranform using training text\n",
        "docs_train_counts = movieVzer.fit_transform(docs_train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzao5FbMiQ_k",
        "outputId": "b7ec7535-46d5-492e-9e6d-48555e4ae0ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 'screen' is found in the corpus, mapped to index 2290\n",
        "movieVzer.vocabulary_.get('screen')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH9BOXgYiS0k",
        "outputId": "827b202f-a181-49bd-d6b4-55b0b4ee837f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Likewise, Mr. Steven Seagal is present...\n",
        "movieVzer.vocabulary_.get('seagal')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj9d5ZheiUaG",
        "outputId": "45e90abd-b192-4198-d31d-55134c4a8602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# huge dimensions! 1,600 documents, 3K unique terms.\n",
        "docs_train_counts.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8529, 3000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0TEvaVgiV8h"
      },
      "source": [
        "# Convert raw frequency counts into TF-IDF values\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "movieTfmer = TfidfTransformer()\n",
        "docs_train_tfidf = movieTfmer.fit_transform(docs_train_counts)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvjymJWLiXsi"
      },
      "source": [
        "# Using the fitted vectorizer and transformer, transform the test data\n",
        "docs_test_counts = movieVzer.transform(docs_test)\n",
        "docs_test_tfidf = movieTfmer.transform(docs_test_counts)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGz9zFW3iZwU",
        "outputId": "a68ed262-a9fa-47d7-9f89-7e58decd18c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now ready to build a classifier.\n",
        "# We will use Multinominal Naive Bayes as our model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Train a Multimoda Naive Bayes classifier. Again, we call it \"fitting\"\n",
        "clf = MultinomialNB()\n",
        "clf.fit(docs_train_tfidf, y_train)\n",
        "# Predict the test set results, find accuracy\n",
        "y_pred = clf.predict(docs_test_tfidf)\n",
        "print(\"Accuracy:\\n\", round(accuracy_score(y_test, y_pred),2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:\n",
            " 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXgQuDbMicrV",
        "outputId": "b52f4c0d-8250-40c7-b21b-d26eac06e80d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Provide the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm=cm[::-1, ::1]\n",
        "print(\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[260 797]\n",
            " [804 272]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44xZnGXYieo6",
        "outputId": "c06efe8d-1fa1-4267-d668-10f857a9129f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Provide the Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.75      0.75      1076\n",
            "           1       0.75      0.75      0.75      1057\n",
            "\n",
            "    accuracy                           0.75      2133\n",
            "   macro avg       0.75      0.75      0.75      2133\n",
            "weighted avg       0.75      0.75      0.75      2133\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KrnO4LPigo-",
        "outputId": "0fa88969-3522-4e57-db35-ad0544b74139",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Using GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(class_weight=\"balanced\", random_state=0)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(logreg, param_grid, scoring=\"roc_auc\", cv=5)\n",
        "grid_train = grid.fit(docs_test_tfidf , y_test)\n",
        "pred_grid = grid_train.predict(docs_test_tfidf)\n",
        "confusion = confusion_matrix(y_test, pred_grid)\n",
        "cm=confusion\n",
        "cm=cm[::-1,::1]\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[ 95 962]\n",
            " [964 112]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lba2bZUgiigp",
        "outputId": "64d8d42f-3cb1-4963-fa79-b751db1071d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Accuracy:\\n\", round(accuracy_score(y_test, pred_grid),3))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:\n",
            " 0.903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oAz9SeDimPf",
        "outputId": "800a04ca-d1c9-45db-a107-373f40b05480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Best Estimator:\\n\", grid.best_estimator_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Estimator:\n",
            " LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2MGdXP8ioms",
        "outputId": "f3001daa-c81e-4054-fbd7-0f6a62e143c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Best Score:\\n\", (grid.best_score_))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score:\n",
            " 0.7551331366677749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uksedqu_irG3",
        "outputId": "9cb8906e-5687-4c63-b0ac-c5862325660c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Best Parameter:\\n\", grid.best_params_)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Parameter:\n",
            " {'C': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUzOnaPFitHX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}